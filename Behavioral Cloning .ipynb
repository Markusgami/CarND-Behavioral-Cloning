{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Behavioral Cloning Code Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First , I need to import all model I needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Input,Cropping2D,Lambda,Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "from keras.layers import Lambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Read the data from our csv log file and save the information in a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data/driving_log.csv')\n",
    "data['center']  = data['center'].apply(lambda x: 'data/data/' + x )\n",
    "data['left']  = data['left'].apply(lambda x: 'data/data/'+ x.strip() )\n",
    "data['right']  = data['right'].apply(lambda x: 'data/data/'+ x.strip() )\n",
    "\n",
    "columns = ['center', 'left' , 'right', 'steering']\n",
    "new_data = data[columns]\n",
    "new_data.to_csv('new_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the new file. I will get 10% of the data as valid data. The rest data will be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('new_data.csv')\n",
    "new_data = shuffle(new_data)\n",
    "n = len(new_data)\n",
    "train_data = new_data[:int(0.9*n)]\n",
    "valid_data = new_data[int(0.9*n):]\n",
    "\n",
    "x_valid = valid_data['center'].values\n",
    "y_valid = valid_data['steering'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I will use images from all cameras. For image from left cameras, I will add the steering angel by 0.25. For the right cameras, I will do the oppsite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "m = len(train_data)\n",
    "adjust_angel = 0.25\n",
    "image_size = [160,320,3]\n",
    "\n",
    "x_train  = np.zeros((3*m),dtype = 'O')\n",
    "y_train  = np.zeros((3*m))\n",
    "\n",
    "x_train[ : m] = train_data.center.values \n",
    "y_train[ : m] = train_data.steering.values\n",
    "\n",
    "x_train[ m: 2*m] = train_data.left.values \n",
    "y_train[ m: 2*m] = train_data.steering.values + adjust_angel\n",
    "\n",
    "x_train[ 2*m: 3*m] = train_data.right.values \n",
    "y_train[ 2*m: 3*m] = train_data.steering.values - adjust_angel\n",
    "\n",
    "x_train, y_train = shuffle(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are the fuctions I used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#To resize and normalize the image.\n",
    "def preprocess_image(img):\n",
    "    from keras.backend import tf as ktf\n",
    "    img = ktf.image.resize_images(img, (64, 64) )\n",
    "    img = 2* ((img / 255) - 0.5)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Generator for validation\n",
    "def get_valid_batch(x, y, batch_size = 32):\n",
    "    num_batch = len(x)// batch_size\n",
    "    x_batch = np.zeros((batch_size, image_size[0],image_size[1],image_size[2]))\n",
    "    x = x[:num_batch * batch_size]\n",
    "    y = y[:num_batch * batch_size]\n",
    "    while 1:       \n",
    "        for i in range(num_batch):\n",
    "            files = x[i * batch_size :(i+1) * batch_size]\n",
    "            y_batch = y[i * batch_size :(i+1) * batch_size]   \n",
    "            for j in range(batch_size):\n",
    "                #img = np.array(Image.open(files[j]))\n",
    "                img = cv2.imread(files[j])\n",
    "                img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)                \n",
    "                x_batch[j] = img                    \n",
    "            yield  x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get more training data, my generator for training is  a bit complex. \n",
    "I used a lot image augumentation. \n",
    "I just random pick the images from my training set. \n",
    "Then I will randomly flip , change brightness, and shift the image. For the shifted image, I will compensate the angel.\n",
    "And because most of time for our driving, angel is zero. So i just drop some of the data if it's angel is less than 0.1. \n",
    "This drop probability will be reduced as the training epoch rise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Generator for training.\n",
    "shred = 100\n",
    "def get_train_batch(x, y, batch_size = 32):\n",
    "    x , y = shuffle(x, y)\n",
    "    x_batch = np.zeros((batch_size, image_size[0],image_size[1],image_size[2]))\n",
    "    y_batch = np.zeros((batch_size))\n",
    "    while 1:       \n",
    "        for i in range(batch_size):\n",
    "            keep_prob = 0\n",
    "            while keep_prob == 0:\n",
    "                #random pick 1 data from my whole data set\n",
    "                index = random.randint(0,len(x)) -1\n",
    "                files, angel = x[index], y[index]\n",
    "                img = cv2.imread(files)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)      \n",
    "                #data augumentaion            \n",
    "                img, angel = random_flip(img, angel) #random flip            \n",
    "                \n",
    "\n",
    "                if abs(angel) < 0.1:\n",
    "                    jj = random.randint(0,100)\n",
    "                    if jj >shred:\n",
    "                        keep_prob = 1\n",
    "                else:\n",
    "                    keep_prob = 1\n",
    "            img, angel = shift_image(img, angel) #random shift\n",
    "            img = augment_brightness(img) #random change brightness        \n",
    "            x_batch[i] ,y_batch[i]= img, angel\n",
    "            \n",
    "        yield  shuffle(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def random_flip(img, angel):\n",
    "    jj = random.randint(0,100)\n",
    "    if jj >50:\n",
    "        img = np.fliplr(img)\n",
    "        angel = -1 * angel\n",
    "    return img, angel\n",
    "\n",
    "def augment_brightness(img):\n",
    "    img = np.array(img, dtype = np.float32)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    #img = np.array(img, dtype = np.float32)\n",
    "    random_bright = .5+np.random.uniform()\n",
    "    img[: , : , 2] = img[: , : , 2] * random_bright\n",
    "    img[: , : , 2] [img[: , : , 2]>255] = 255\n",
    "    img = np.array(img,dtype = np.uint8)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_HSV2RGB)\n",
    "    return img\n",
    "\n",
    "def shift_image(image,angel):\n",
    "    shift_x = 100*np.random.uniform() - 50 # x axis use range of +-50 \n",
    "    shift_y = 30*np.random.uniform() - 15 #y axis use range of +-15\n",
    "    angel = angel + shift_x*0.005 # compensate the angel\n",
    "    trans_m = np.float32([[1,0,shift_x],[0,1,shift_y]])\n",
    "    image = cv2.warpAffine(image , trans_m , (320 , 160))\n",
    "    return image, angel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For my architecture, I'm using 5 layers of convolution and 4 layers of dense.  I used dropout layer between dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pic_inputs = Input(shape = (160,320,3,), dtype = 'int32', name = 'pic_input')\n",
    "pic_cropped = Cropping2D(cropping = ((50,30), (0,0)),input_shape = (160,320,3), name = 'pic_cropped')(pic_inputs)\n",
    "pic_resize = Lambda(preprocess_image, name = 'pic_resized')(pic_cropped) #64*64\n",
    "\n",
    "conv1 = Conv2D(filters = 48,kernel_size = (5,5) ,strides = (2,2), padding = 'same', activation = 'relu', name = 'conv1')(pic_resize)  #32*32\n",
    "conv2 = Conv2D(filters = 36,kernel_size = (5,5),strides=(2,2), padding = 'same', activation = 'elu',name = 'conv2')(conv1) #16*16\n",
    "conv3 = Conv2D(filters = 48,kernel_size = (5,5),strides=(2,2),  padding = 'same',activation = 'elu',name = 'conv3')(conv2) #8*8\n",
    "conv4 = Conv2D(filters = 64,kernel_size = (3,3),strides=(2,2),  padding = 'same',activation = 'elu',name = 'conv4')(conv3) #4*4\n",
    "#conv5 = Conv2D(filters = 80,kernel_size = (3,3),strides=(2,2),  padding = 'same',activation = 'elu',name = 'conv5')(conv4) #2*2\n",
    "\n",
    "\n",
    "flat = Flatten()(conv4)\n",
    "\n",
    "dense1 = Dense(100, activation = 'relu', name = 'dense1')(flat)\n",
    "drop_out1 = Dropout(0.2)(dense1)\n",
    "\n",
    "dense2 = Dense(50, activation = 'relu', name = 'dense2')(drop_out1)\n",
    "drop_out2 = Dropout(0.2)(dense2)\n",
    "\n",
    "dense3 = Dense(20, activation = 'relu', name = 'dense3')(drop_out2)\n",
    "drop_out3 = Dropout(0.2)(dense3)\n",
    "\n",
    "steering_angel = Dense(1, activation = None, name = 'output')(drop_out3)\n",
    "\n",
    "model = Model(inputs = pic_inputs , outputs = steering_angel)\n",
    "opt = keras.optimizers.Adam(lr = 0.0001)\n",
    "model.compile(optimizer = opt, loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "My computer went dead when I use Model.fit_generator..I don't know why..So I wrote this function to call modle.train_on_batch to avoid that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    loss_total = 0\n",
    "    data = get_train_batch(x_train, y_train, batch_size = batch_size)\n",
    "    for i in range(num_steps_train):\n",
    "        x_tr, y_tr = next(data)\n",
    "        model.train_on_batch(x_tr, y_tr)\n",
    "        if i%100 == 0 :\n",
    "            loss = model.evaluate(x= x_tr,y = y_tr,verbose = 0)\n",
    "            print ('Batch({}/{}: loss{})'.format(i,num_steps_train,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The loss didn't go down much after 2 epochs of training. So I'm not going to train more and see how this goes.\n",
    "And result is very good. My model is working perfect on track2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Batch(0/1352: loss0.028633666690438986)\n",
      "Batch(100/1352: loss0.021348596084862947)\n",
      "Batch(200/1352: loss0.020161375403404236)\n",
      "Batch(300/1352: loss0.031038138549774885)\n",
      "Batch(400/1352: loss0.021555493585765362)\n",
      "Batch(500/1352: loss0.018212802708148956)\n",
      "Batch(600/1352: loss0.01502295769751072)\n",
      "Batch(700/1352: loss0.03213645378127694)\n",
      "Batch(800/1352: loss0.02592579834163189)\n",
      "Batch(900/1352: loss0.025934227742254734)\n",
      "Batch(1000/1352: loss0.02331148274242878)\n",
      "Batch(1100/1352: loss0.027378769824281335)\n",
      "Batch(1200/1352: loss0.0241343155503273)\n",
      "Batch(1300/1352: loss0.027837557718157768)\n",
      "Train loss:0.027955196213704594\n",
      "Valid loss:0.02482956958313783\n",
      "epoch 2\n",
      "Batch(0/1352: loss0.02223508246243)\n",
      "Batch(100/1352: loss0.019850264070555568)\n",
      "Batch(200/1352: loss0.022905481979250908)\n",
      "Batch(300/1352: loss0.023467320716008544)\n",
      "Batch(400/1352: loss0.02203707443550229)\n",
      "Batch(500/1352: loss0.022404426708817482)\n",
      "Batch(600/1352: loss0.026634466368705034)\n",
      "Batch(700/1352: loss0.023705987259745598)\n",
      "Batch(800/1352: loss0.024842097889631987)\n",
      "Batch(900/1352: loss0.02355285407975316)\n",
      "Batch(1000/1352: loss0.014679836109280586)\n",
      "Batch(1100/1352: loss0.01609600684605539)\n",
      "Batch(1200/1352: loss0.02403764147311449)\n",
      "Batch(1300/1352: loss0.02060781349427998)\n",
      "Train loss:0.027880164601746395\n",
      "Valid loss:0.027454608740905922\n",
      "epoch 3\n",
      "Batch(0/1352: loss0.022069819970056415)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2b31c734cacf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_valid_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_steps_train\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2075a65298f4>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "opt = keras.optimizers.Adam(lr = 0.0001)\n",
    "nb_epoch = 8\n",
    "num_steps_train = (len(x_train) // batch_size) *8\n",
    "num_steps_valid = (len(x_valid) // batch_size) \n",
    "\n",
    "for e in range(nb_epoch):\n",
    "    print(\"epoch {}\".format(e+1))\n",
    "    train_one_epoch()\n",
    "    train_data = get_valid_batch(x_train, y_train, batch_size = batch_size)\n",
    "    train_loss = model.evaluate_generator(train_data, steps = num_steps_train/8)\n",
    "    shred = 100 / (e+1)\n",
    "    print ('Train loss:{}'.format(train_loss))\n",
    "    valid_data = get_valid_batch(x_valid, y_valid, batch_size = batch_size)\n",
    "    valid_loss = model.evaluate_generator(valid_data, steps = num_steps_valid)\n",
    "    print ('Valid loss:{}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.save('model2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is the code which will cause my p2 instance to die after running.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data = get_train_batch(x_train, y_train, batch_size = batch_size)\n",
    "valid_data = get_valid_batch(x_valid, y_valid, batch_size = batch_size)\n",
    "model.fit_generator(generator = train_data, steps_per_epoch= num_steps_train, epochs =1 , validation_data = valid_data, validation_steps = num_steps_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
